@inproceedings{Wilkerson2013TracingTF,
  title={Tracing the Flow of Policy Ideas in Legislatures: a Text Reuse Approach Tracing Policy Ideas},
  author={John Wilkerson and David Smith and Nick Stramp and Wilkerson and Smith and Stramp},
  year={2013}
}

@inproceedings{Burgess2016TheLI,
  title={The Legislative Influence Detector: Finding Text Reuse in State Legislation},
  author={Matthew Burgess and Eugenia Giraudy and Julian Katz-Samuels and Joe Walsh and Derek Willis and Lauren Haynes and Rayid Ghani},
  booktitle={KDD},
  year={2016}
}

@inproceedings{Smith2014DetectingAM,
  title={Detecting and modeling local text reuse},
  author={David A. Smith and Ryan Cordell and Elizabeth Maddock Dillon and Nick Stramp and John Wilkerson},
  booktitle={JCDL},
  year={2014}
}


@inproceedings{Rajaraman2011MiningOM,
  title={Mining of Massive Datasets},
  author={Anand Rajaraman and Jeffrey D Ullman},
  year={2011}
}

@inproceedings{bamman2015open,
  title={Open extraction of fine-grained political statements},
  author={Bamman, David and Smith, Noah A}
}

@inproceedings{gerrish2011predicting,
  title={Predicting legislative roll calls from text},
  author={Gerrish, Sean and Blei, David M},
  booktitle={Proceedings of the 28th international conference on machine learning (icml-11)},
  pages={489--496},
  year={2011}
}

@article{simfriends,
  title={Friends with Motives: Using Text to Infer Influence on SCOTUS},
  author={Sim, Yanchuan and Routledge, Bryan R and Smith, Noah A}
}

@inproceedings{gerrish2012they,
  title={How they vote: Issue-adjusted models of legislative behavior},
  author={Gerrish, Sean and Blei, David M},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2753--2761},
  year={2012}
}

@article{Blei2016,
abstract = {One of the core problems of modern statistics is to approximate difficult-to-compute probability densities. This problem is especially important in Bayesian statistics, which frames all inference about unknown quantities as a calculation involving the posterior density. In this paper, we review variational inference (VI), a method from machine learning that approximates probability densities through optimization. VI has been used in many applications and tends to be faster than classical methods, such as Markov chain Monte Carlo sampling. The idea behind VI is to first posit a family of densities and then to find the member of that family which is close to the target. Closeness is measured by Kullback-Leibler divergence. We review the ideas behind mean-field variational inference, discuss the special case of VI applied to exponential family models, present a full example with a Bayesian mixture of Gaussians, and derive a variant that uses stochastic optimization to scale up to massive data. We discuss modern research in VI and highlight important open problems. VI is powerful, but it is not yet well understood. Our hope in writing this paper is to catalyze statistical research on this class of algorithms.},
archivePrefix = {arXiv},
arxivId = {arXiv:1601.00670v4},
author = {Blei, David M and Kucukelbir, Alp and Mcauliffe, Jon D},
eprint = {arXiv:1601.00670v4},
file = {:Users/saul/Library/Application Support/Mendeley Desktop/Downloaded/Blei, Kucukelbir, Mcauliffe - 2016 - Variational Inference A Review for Statisticians(2).pdf:pdf},
keywords = {Algorithms,Computationally Intensive Methods,Statistical Computing},
mendeley-groups = {PGM},
title = {{Variational Inference: A Review for Statisticians}},
year = {2016}
}


@article{10.7717/peerj-cs.55,
 title = {Probabilistic programming in Python using PyMC3},
 author = {Salvatier, John and Wiecki, Thomas V. and Fonnesbeck, Christopher},
 year = 2016,
 month = apr,
 keywords = {Bayesian statistic, Probabilistic Programming, Python, Markov chain Monte Carlo, Statistical modeling},
 abstract = {
            Probabilistic programming allows for automatic Bayesian inference on user-defined probabilistic models. Recent advances in Markov chain Monte Carlo (MCMC) sampling allow inference on increasingly complex models. This class of MCMC, known as Hamiltonian Monte Carlo, requires gradient information which is often not readily available. PyMC3 is a new open source probabilistic programming framework written in Python that uses Theano to compute gradients via automatic differentiation as well as compile probabilistic programs on-the-fly to C for increased speed. Contrary to other probabilistic programming languages, PyMC3 allows model specification directly in Python code. The lack of a domain specific language allows for great flexibility and direct interaction with the model. This paper is a tutorial-style introduction to this software package.
         },
 volume = 2,
 pages = {e55},
 journal = {PeerJ Computer Science},
 issn = {2376-5992},
 url = {https://doi.org/10.7717/peerj-cs.55},
 doi = {10.7717/peerj-cs.55}
}

@article{carroll2009comparing,
  title={Comparing NOMINATE and IDEAL: Points of difference and Monte Carlo tests},
  author={Carroll, Royce and Lewis, Jeffrey B and Lo, James and Poole, Keith T and Rosenthal, Howard},
  journal={Legislative Studies Quarterly},
  volume={34},
  number={4},
  pages={555--591},
  year={2009},
  publisher={Wiley Online Library}
}

@inproceedings{mccarty2011measuring,
  title={Measuring legislative preferences},
  author={McCarty, Nolan}
}

@article{martin2002dynamic,
  title={Dynamic ideal point estimation via Markov chain Monte Carlo for the US Supreme Court, 1953--1999},
  author={Martin, Andrew D and Quinn, Kevin M},
  journal={Political Analysis},
  volume={10},
  number={2},
  pages={134--153},
  year={2002},
  publisher={SPM-PMSAPSA}
}